import tensorflow as tf
import numpy as np
import os
import pandas as pd
from os.path import exists
import tensorflow.keras as keras
from tensorflow.keras import layers
from tensorflow.keras.layers import Concatenate
from tensorflow.keras import layers
from tensorflow.keras.layers import Input
import time
from tensorflow.keras.models import Model

from DataUploader import UploadData
#tf.config.run_functions_eagerly(True)

class MasterGAN:

    def __init__(self,loaded, blackbox, in_h, percentage , sample_name, img_w=256, img_h=256):
        self.blackbox = blackbox
        self.img_width = img_w
        self.img_height = img_h
        self.EPOCHS = 100
        self.noise_dim = 100
        self.loaded = loaded
        self.num_examples_to_generate = 16
        self.in_height = in_h
        self.noise_generator = tf.random.Generator.from_non_deterministic_state()
        self.sample_name = sample_name
        # CREATE AND COMPILE DISCRIMINATOR
        self.substitute_detector = self.build_substitute_detector_model()
        self.substitute_detector_optimizer = tf.keras.optimizers.Adam(learning_rate=2e-3)#prima 3e-3 e 1e-3
        self.substitute_detector.compile(optimizer = self.substitute_detector_optimizer, loss = 'binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()], run_eagerly = None)
        

        # CREATE GENERATOR
        self.percentage = percentage
        self.noise_height = int(self.in_height  * percentage)
        self.generator = self.build_generative_model()
        self.generator_optimizer = tf.keras.optimizers.Adam(learning_rate=1e-3)
        
        # The generator takes a malware sample and noise as input and generates adversarial malware samples
        example = Input(shape=(self.in_height,512, 1))
        noise_ = Input(shape=(100))
        input = [example, noise_]
        malware_examples = self.generator(input)
        self.set_trainable(self.generator,True)
        self.set_trainable(self.substitute_detector,False)
        validity = self.substitute_detector(malware_examples)

        #CREATE AND COMPILE THE COMBINED MODEL
        self.combined = Model(input, validity)
        self.combined.compile(optimizer=self.generator_optimizer , loss= 'binary_crossentropy', metrics=['accuracy', tf.keras.metrics.Precision(), tf.keras.metrics.Recall()])
    
        #self.seed = tf.random.normal([self.num_examples_to_generate, self.noise_dim])
        self.cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=False)

        # metrics array for each epoch 
        self.gen_loss_array, self.sub_det_loss_array = [],[]
        self.gen_acc_array, self.sub_det_acc_array = [],[]
        self.gen_prec_array, self.sub_det_prec_array = [],[]
        self.gen_rec_array, self.sub_det_rec_array = [],[]

        # metrics array for each batch 
        self.gen_loss_batch_array, self.sub_det_loss_batch_array,  = [],[]
        self.gen_acc_batch_array, self.sub_det_acc_batch_array = [],[]
        self.gen_prec_batch_array, self.sub_det_prec_batch_array,  = [],[]
        self.gen_rec_batch_array, self.sub_det_rec_batch_array = [],[]
    
    def set_trainable(self, model, trainable):
        model.trainable = trainable
        for layer in model.layers: layer.trainable = trainable

    def build_substitute_detector_model(self):

      substitute_detector = tf.keras.Sequential()
      substitute_detector.add(tf.keras.layers.experimental.preprocessing.Resizing(256, 256, interpolation="bilinear"))
      substitute_detector.add(layers.Conv2D(64, (5, 5), activation='relu', strides=(2, 2), padding='same',
                                            input_shape=[self.img_width, self.img_height, 1]))

      substitute_detector.add(layers.BatchNormalization())

      substitute_detector.add(layers.Conv2D(64, (4, 4), strides=(2, 2), padding='same'))
      substitute_detector.add(layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'))      
      substitute_detector.add(layers.BatchNormalization())

      substitute_detector.add(layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'))      
      substitute_detector.add(layers.BatchNormalization())
      substitute_detector.add(layers.LeakyReLU())

      substitute_detector.add(layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'))
      substitute_detector.add(layers.BatchNormalization())
      substitute_detector.add(layers.LeakyReLU())

      substitute_detector.add(layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'))
      substitute_detector.add(layers.BatchNormalization())
      substitute_detector.add(layers.LeakyReLU())

      substitute_detector.add(layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'))
      substitute_detector.add(layers.BatchNormalization())
      substitute_detector.add(layers.LeakyReLU())

      substitute_detector.add(layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'))
      substitute_detector.add(layers.BatchNormalization())
      substitute_detector.add(layers.LeakyReLU())

      substitute_detector.add(layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'))
      substitute_detector.add(layers.BatchNormalization())
      substitute_detector.add(layers.LeakyReLU())

      substitute_detector.add(layers.Conv2D(32, (4, 4), strides=(2, 2), padding='same'))
      substitute_detector.add(layers.BatchNormalization())
      substitute_detector.add(layers.LeakyReLU())


      substitute_detector.add(layers.Flatten())
      substitute_detector.add(layers.Dense(256, activation='relu'))
      substitute_detector.add(layers.Dropout(0.2))
      substitute_detector.add(layers.Dense(64, activation='relu'))
      substitute_detector.add(layers.Dropout(0.5))
      substitute_detector.add(layers.Dense(32, activation='relu'))
      substitute_detector.add(layers.Dropout(0.2))
      substitute_detector.add(layers.Dense(16, activation='relu'))
      substitute_detector.add(layers.Dropout(0.2))

      substitute_detector.add(layers.Dense(1, activation='sigmoid'))

      return substitute_detector

    def pre_train_sd(self,dest):      
      uploader = UploadData() 
      
      # uploading the dataset of images related to malicious files (already converted using a colormap)
    
      mal_test = uploader.upload_test_mal_set_no_cm()

      # uploading the dataset of images related to benign files (already converted using a colormap)
      ben_test = uploader.upload_test_ben_set_no_cm()

      # created the mixed dataset to train
      mixed_test = uploader.upload_test_set(ben_test,mal_test)
      
      epoch=0
      log = [[1,0,0,0]]
      fscore=0

      while(log[0][0]>0.2 or log[0][1]<0.8 or fscore<0.9):
        log = []
        epoch+=1
        for samples in mixed_test:
          bb_prediction = self.blackbox.make_prediction(np.array(samples), epoch, 111111, '/home/user/projects/Code/GAN_TRAINING_RESULTS/sub_det_pre_train_mid' ) 

          self.set_trainable(self.substitute_detector, True)
          log.append(self.substitute_detector.train_on_batch(samples, np.round(bb_prediction).astype(int) ))
          
        log = np.mean(log, axis=0, keepdims=True)
        print(log)
        pd.DataFrame(log).to_csv(dest,mode='a',header = False)
        print(log[0][0])
        print(log[0][1])
        fscore = 2*((log[0][2]*log[0][3])/(log[0][2]+log[0][3]))
      self.set_trainable(self.substitute_detector,  False)
      
      

    def build_generative_model(self):
    
      noise_input = Input(shape=(100)) 
      
      noise_output = layers.Dense(128*16*1, use_bias=False)(noise_input)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)

      noise_output = layers.Reshape((16, 128, 1))(noise_output)

      noise_output = tf.keras.layers.UpSampling2D(  size=(4, 4), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (4, 4), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)

      noise_output = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (16, 16), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)
      noise_output = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (16, 16), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)
      noise_output = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (16, 16), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)
      noise_output = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (16, 16), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)
      noise_output = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (16, 16), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)
      noise_output = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (16, 16), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)
      noise_output = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (16, 16), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)
      noise_output = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (32, 32), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)

      noise_output = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (4, 4), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)
      
      noise_output = tf.keras.layers.UpSampling2D(size=(2, 2), interpolation='nearest')(noise_output)
      noise_output = layers.Conv2D(16, (32, 32), strides=(2, 2), padding='same', use_bias=False)(noise_output)
      noise_output = layers.BatchNormalization()(noise_output)
      noise_output = layers.LeakyReLU()(noise_output)

      noise_output = layers.Conv2DTranspose(1, (4, 4), strides=(2, 2), padding='same', use_bias=False, activation='tanh')(noise_output)

      old_min = tf.reduce_min(noise_output)
      old_max = tf.reduce_max(noise_output)
      noise_output = ((noise_output - old_min)*(255-0))/(old_max-old_min)
      #noise_output = tf.keras.layers.experimental.preprocessing.Resizing( self.in_height, 256, interpolation="bilinear",)(noise_output)
      #noise_output = ((noise_output+1)*255)/2
      noise_output = tf.keras.layers.experimental.preprocessing.Resizing(self.noise_height, 512, interpolation='bilinear')(noise_output)
      malware_input = Input(shape = (self.in_height,512,1))
      final_output = Concatenate(axis=1)([malware_input, noise_output])

      generator = Model(inputs=[malware_input,noise_input], outputs=[final_output])
      return generator

    def create_result_folder(self,root):
      result_dir = os.path.join(root, self.sample_name)

      if self.percentage==0.4:
        os.mkdir(result_dir)
      result_dir = os.path.join(result_dir, str(self.percentage*100)+'%')
      os.mkdir(result_dir)

      data_dir = os.path.join(result_dir, 'TR_NO_CM')
      os.mkdir(data_dir)

      data_dir = os.path.join(result_dir, 'TR_CM')
      os.mkdir(data_dir)

      data_dir = os.path.join(result_dir, 'npy')
      os.mkdir(data_dir)
      return result_dir

    def train(self,ben_ds, malware_sample,test_mal_path_exes, bb_on_real_):
      root = '/home/user/projects/Code/GAN_TRAINING_RESULTS/results/'
      
      result_dir = self.create_result_folder(root)

      # before starting the training the sample is submitted to the blackbox to see if it is classified as 
      # benign or malicious, if it is already classified as benign there is no need for training
      bb_on_original = self.blackbox.make_prediction(malware_sample, 9999, 1, result_dir) 
      bb_on_original = np.around(bb_on_original)

      if (bb_on_original.astype(int)  == np.zeros((64,1))).all():
        with open(root+"/log.txt", "a") as myfile:
          myfile.write('\n'+str(bb_on_original.flatten()))
          myfile.write("\n sample " + self.sample_name +" was originally classified as benign by the blackbox detector, training is not needed") 
        with open(str(result_dir)+"/log_" + self.sample_name + ".txt", "a") as myfile:
          myfile.write('\n'+str(bb_on_original.flatten()))
          myfile.write("t\n he sample was originally classified as benign by the blackbox detector, training is not needed") 
      
      else:

        with open(root+"/log.txt", "a") as myfile:
          myfile.write('\n'+str(bb_on_original.flatten()))
          myfile.write("\n sample " + self.sample_name +" was classified as malicious by the blackbox detector") 
        with open(str(result_dir)+"/log_" + self.sample_name + ".txt", "a") as myfile:
          myfile.write('\n'+str(bb_on_original.flatten()))
          myfile.write("\n the sample was originally classified as malicious by the blackbox detector") 
        
        sd_train_log = str(result_dir)+"/log_sd_" + self.sample_name + ".csv"
        df_list = []
        samples_created = 0
        successful_samples = 0

        sub_tr_adv = True
        sub_train_ben = True
        gen_train =True
        self.pre_train_sd(sd_train_log)
        self.substitute_detector.save(str(result_dir)+'/sd_pretrained.h5')
        for epoch in range(self.EPOCHS):
          samples_created_epoch = 0
          successful_samples_epoch = 0

          ## Training metrics ##
          # generator loss for the current epoch:
          gen_loss = 0
          # generator accuracy for the current epoch:
          gen_acc = 0
          gen_prec = 0
          gen_rec = 0
          # discriminator loss for the current epoch:
          sub_det_loss = 0
          # discriminator accuracy for the current epoch:
          sub_det_acc = 0 
          sub_det_prec = 0
          sub_det_rec = 0
          # counter defined to count the number of batches
          counter = 0

          batch_success = [0,0,0,0]
          sub_loss_ben = 0
          sub_acc_ben = 0

          for ben_batch, bb_on_real in zip(ben_ds,bb_on_real_):   

            counter = counter + 1
            # a new batch of noise of size 100 is created at the beginning of every epoch
            new_noise = self.noise_generator.normal(shape=[64,100])
            # the generator is used to create a new batch of malware samples
            adv_noise_samples = self.generator.predict([malware_sample, new_noise])
            
            samples_created_epoch = samples_created_epoch + 64
            samples_created = samples_created + 64
              
            # the blackbox detector is used to make prediction on the new adversarial samples
            bb_on_adv = self.blackbox.make_prediction(adv_noise_samples, epoch, counter, result_dir) 
          
            bb_on_adv_lbl = np.round(bb_on_adv).astype(int)  
            unique, counts = np.unique(bb_on_adv_lbl.flatten(), return_counts=True)
            misclassified = dict(zip(unique, counts))
            
            if sub_tr_adv:
              print("training on adv resumed")
              self.set_trainable(self.substitute_detector, True)
              sub_loss_now_adv, sub_acc_now_adv, sub_prec_now_adv, sub_rec_now_adv = self.substitute_detector.train_on_batch(adv_noise_samples, bb_on_adv_lbl)
            else:
              print("training on adv halted")
              sub_loss_now_adv, sub_acc_now_adv, sub_prec_now_adv, sub_rec_now_adv = self.substitute_detector.evaluate(adv_noise_samples,np.round(bb_on_adv).astype(int))
              
              
            if sub_train_ben == True:
              print("training on ben resumed")
              self.set_trainable(self.substitute_detector, True)
              sub_loss_now_ben, sub_acc_now_ben, sub_prec_now_ben, sub_rec_now_ben = self.substitute_detector.train_on_batch(ben_batch,np.round(bb_on_real).astype(int))
              
            else:
              print("training on ben halted")
              sub_loss_now_ben, sub_acc_now_ben, sub_prec_now_ben, sub_rec_now_ben = self.substitute_detector.evaluate(ben_batch,np.round(bb_on_real).astype(int))
            
            sub_loss_now = (sub_loss_now_adv + sub_loss_now_ben)/2
            sub_acc_now = (sub_acc_now_adv + sub_acc_now_ben)/2
            sub_prec_now = (sub_prec_now_adv + sub_prec_now_ben)/2
            sub_rec_now = (sub_rec_now_adv + sub_rec_now_ben)/2

            sub_loss_ben += sub_loss_now_ben
            sub_acc_ben +=  sub_acc_now_ben 

            successful_samples_epoch = successful_samples_epoch + misclassified.get(0,0)   
            successful_samples = successful_samples + misclassified.get(0,0)  

            if (bb_on_adv_lbl == np.zeros((64,1))).all():
              batch_success[counter-1] = 1
            else:
              batch_success[counter-1] = 0

            self.set_trainable(self.substitute_detector, False)
            
            ## Generator training ##
            # another batch of noise data is created to train the generator
            new_noise_1 = self.noise_generator.normal(shape=[64,100])
            if gen_train:
              gen_loss_now, gen_acc_now, gen_prec_now, gen_rec_now = self.combined.train_on_batch([malware_sample, new_noise_1], np.zeros((64,1)))
            else:
              gen_loss_now, gen_acc_now, gen_prec_now, gen_rec_now = self.combined.evaluate([malware_sample, new_noise_1], np.zeros((64,1)))


            gen_loss = gen_loss + gen_loss_now
            gen_acc = gen_acc + gen_acc_now
            gen_prec = gen_prec + gen_prec_now
            gen_rec = gen_rec + gen_rec_now

            log_by_batch_adv = ''
            log_by_batch_ben = ''
            
            sub_det_loss = sub_det_loss + sub_loss_now
            sub_det_acc =  sub_det_acc + sub_acc_now
            sub_det_prec = sub_det_prec + sub_prec_now
            sub_det_rec =  sub_det_rec + sub_rec_now

            log_by_batch_adv = '\n log_by_batch_adv:  sub_det_loss_batch_adv = {0:.3f}, sub_det_acc_batch_adv = {1:.3f}'.format( sub_loss_now_adv, sub_acc_now_adv)
            log_by_batch_ben = '\n log_by_batch_ben: sub_det_loss_batch_ben = {0:.3f}, sub_det_acc_batch_ben = {1:.3f}'.format( sub_loss_now_ben, sub_acc_now_ben)
            
            self.sub_det_loss_batch_array.append(sub_loss_now)
            self.sub_det_acc_batch_array.append(sub_acc_now)
            self.sub_det_prec_batch_array.append(sub_prec_now)
            self.sub_det_rec_batch_array.append(sub_rec_now)

            log_by_batch = '\n log_by_batch: gen_loss_batch = {0:.3f}, gen_acc_batch = {1:.3f}, gen_prec_batch = {2:.3f}, gen_rec_batch = {3:.3f}, sub_det_loss_batch = {4:.3f}, sub_det_acc_batch = {5:.3f}, sub_det_prec_batch = {6:.3f}, sub_det_rec_batch = {7:.3f}'.format(gen_loss_now, gen_acc_now, gen_prec_now, gen_rec_now, sub_loss_now, sub_acc_now, sub_prec_now, sub_rec_now)
            
            
            print(log_by_batch)
            print(log_by_batch_adv)
            print(log_by_batch_ben)

            with open(str(result_dir)+"/log_by_batch.txt", "a") as myfile:
              myfile.write("\n BB on adv "+ str(bb_on_adv.flatten()[0:63]))
              myfile.write(log_by_batch) 
            
            self.gen_loss_batch_array.append(gen_loss_now)
            self.gen_acc_batch_array.append(gen_acc_now)
          
          
          gen_loss_epoch = gen_loss/counter
          gen_acc_epoch = gen_acc/counter
          gen_prec_epoch = gen_prec/counter
          gen_rec_epoch = gen_rec/counter

          sub_det_loss_epoch = sub_det_loss/counter
          sub_det_acc_epoch = sub_det_acc/counter 
          sub_det_prec_epoch = sub_det_prec/counter
          sub_det_rec_epoch = sub_det_rec/counter 

          sub_loss_ben_epoch= sub_loss_ben/counter
          sub_acc_ben_epoch = sub_acc_ben/counter

          # print save log for current epoch
          log_by_epoch = '\n gen_loss = {0:.3f}, gen_acc = {1:.3f}, gen_prec = {2:.3f}, gen_rec = {3:.3f}, sub_det_loss = {4:.3f}, sub_det_acc = {5:.3f}, sub_det_prec = {6:.3f}, sub_det_rec = {7:.3f}'.format(gen_loss_epoch, gen_acc_epoch, gen_prec_epoch, gen_rec_epoch, sub_det_loss_epoch, sub_det_acc_epoch, sub_det_prec_epoch, sub_det_rec_epoch)
          
          print(log_by_epoch)


          if ((sub_loss_ben_epoch<0.2) and (sub_acc_ben_epoch >0.8)): 
            sub_train_ben = False
          else:
            sub_train_ben = True
          
          if ((sub_det_loss_epoch<0.2) and (sub_det_acc_epoch>0.8)): 
            gen_train = True
          else:
            gen_train = False
          
          ######### FINAL VALIDATION #########
          new_noise = self.noise_generator.normal(shape=[64,100])
          # the generator is used to create a new batch of malware samples
          val_adv_samples = self.generator.predict([malware_sample, new_noise])
                    
          # the blackbox detector is used to make prediction on the new adversarial samples
          bb_val = self.blackbox.make_prediction(val_adv_samples, 1000, 1, result_dir) 
          
          val_adv = self.substitute_detector.evaluate(val_adv_samples,np.round(bb_val).astype(int))
          print (val_adv)
          if ((val_adv[0] < 0.15) and (val_adv[1] >0.8)) :
            sub_tr_adv = False
          else:
            sub_tr_adv = True
            
          # append metrics for the current epoch
          self.gen_loss_array.append((gen_loss_epoch))
          self.sub_det_loss_array.append((sub_det_loss_epoch))
          self.gen_acc_array.append((gen_acc_epoch))
          self.sub_det_acc_array.append((sub_det_acc_epoch))


          with open(str(result_dir)+"/log_by_epoch.txt", "a") as myfile:
            myfile.write(log_by_epoch) 
            
          if (batch_success == np.ones(4)).all() and (sub_det_acc_epoch)>0.8 and (sub_det_loss_epoch)<0.2:
            final_log = '\n training ended at epoch {0} for sample {fname}'.format(epoch, fname = self.sample_name )
            with open(root+"/log.txt", "a") as myfile:
              myfile.write(final_log) 
            with open(str(result_dir)+"/log_" + self.sample_name + ".txt", "a") as myfile:
              myfile.write(final_log)   
            #break

          df_list.append([self.sample_name, self.percentage, epoch, samples_created, successful_samples, samples_created_epoch, successful_samples_epoch])
        
        df = pd.DataFrame(df_list, columns=['sample_name', 'percentage','epoch','samples_created', 'successful_samples', 'samples_created_epoch', 'successful_samples_epoch'])
        df.to_csv(path_or_buf = result_dir + '/data.csv', sep='|')
        
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_combined_loss.npy',np.array(self.gen_loss_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_sub_det_loss.npy',np.array(self.sub_det_loss_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_combined_acc.npy',np.array(self.gen_acc_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_sub_det_acc.npy',np.array(self.sub_det_acc_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_combined_prec.npy',np.array(self.gen_prec_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_sub_det_prec.npy',np.array(self.sub_det_prec_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_combined_rec.npy',np.array(self.gen_rec_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_sub_det_rec.npy',np.array(self.sub_det_rec_array))

        np.save(str(result_dir)+'/npy/'+self.sample_name+'_combined_loss_batch.npy',np.array(self.gen_loss_batch_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_sub_det_loss_batch.npy',np.array(self.sub_det_loss_batch_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_combined_acc_batch.npy',np.array(self.gen_acc_batch_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_sub_det_acc_batch.npy',np.array(self.sub_det_acc_batch_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_combined_prec_batch.npy',np.array(self.gen_prec_batch_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_sub_det_prec_batch.npy',np.array(self.sub_det_prec_batch_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_combined_rec_batch.npy',np.array(self.gen_rec_batch_array))
        np.save(str(result_dir)+'/npy/'+self.sample_name+'_sub_det_rec_batch.npy',np.array(self.sub_det_rec_batch_array))

        self.generator.save(str(result_dir)+'/generator.h5')
        self.substitute_detector.save(str(result_dir)+'/sub.h5')
        self.combined.save(str(result_dir)+'/comb.h5')
    

    

    def clear_folder(self,mid_dir):
      for f in os.listdir(mid_dir):
        os.remove(os.path.join(mid_dir, f))